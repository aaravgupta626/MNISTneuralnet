# -*- coding: utf-8 -*-
"""MNIST Neural Network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tiy5sq1lZ3wnolYIhYT4hNcWcngF2SiQ
"""

import pandas as pd, matplotlib.pyplot as plt, numpy as np

def tanh(x):
  return np.tanh(x)

def d_tanh(x):
  return 1 - np.square(np.tanh(x))

def sigmoid(x):
  return 1/(1 + np.exp(-x))

def d_sigmoid(x):
  return (1 - sigmoid(x)) * sigmoid(x)

def logloss(y , a):
  return -(y*np.log(a) + (1-y) * np.log(1-a))

def d_logloss(y , a):
  return (a-y) / (a*(1 - a))

def leaky_relu(x):
  temp = []
  for i in A:
    t = []
    for j in i:
      if j>0:
        t.append(j)
      else:
        t.append(0.01*j)
    temp.append(t)
  return np.array(temp)

def d_lrelu(x, alpha=.01):
  temp = []
  for i in A:
    t = []
    for j in i:
      if j>=0:
        t.append(1)
      else:
        t.append(0.01)
    temp.append(t)
  return np.array(temp)


def relu(x):
   return np.maximum(x, 0)

def reluDerivativeSingleElement(xi):
    if xi > 0:
        return 1
    elif xi <= 0:
        return 0

def d_relu(x):
  lst = []
  for i in x:
    temp = []
    for j in i:
      l = reluDerivativeSingleElement(j)
      temp.append(l)
    lst.append(temp)
  return np.array(lst)

class Layer:
  lr = 0.0008

  def __init__(self, input, neur, act, d_act):
    self.W = np.random.randn(neur, input)
    self.B = np.zeros((neur, 1))
    self.act = act
    self.d_act = d_act

  def feed_forward(self, inputs):
    self.a_prev = inputs
    self.Z = np.dot(self.W, self.a_prev) + self.B
    self.a = self.act(self.Z)
    return self.a

  def back_propagate(self, dA):
    dC = np.multiply(self.d_act(self.Z), dA)
    dW = 1/dC.shape[0]*np.dot(dC, self.a_prev.T)
    dB = 1/dC.shape[0]*np.sum(dC, axis=1, keepdims=True)
    dA_prev = np.dot(self.W.T, dC)

    self.W = self.W - self.lr * dW
    self.B = self.B - self.lr * dB

    return dA_prev

from keras.datasets import mnist
from matplotlib import pyplot

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(60000, 784)
x_train = (x_train - np.mean(x_train))/np.std(x_train)

print('X_train: ' + str(x_train.shape))
print('Y_train: ' + str(y_train.shape))
print('X_test:  '  + str(x_test.shape))
print('Y_test:  '  + str(y_test.shape))

y_train_1 = np.zeros((y_train.shape[0],10))
for i in range(y_train.shape[0]):
  y_train_1[i][y_train[i]] = 1
y_train = y_train_1
print(y_train[0])

m = 60000
epochs = 100

layers = [Layer(784, 256, tanh, d_tanh), Layer(256, 32, tanh, d_tanh), Layer(32, 10, sigmoid, d_sigmoid)]
costs = []

for epoch in range(epochs):
    A = x_train.T
    for layer in layers:
        A = layer.feed_forward(A)
    print(epoch)

    cost = 1/m * np.sum(logloss(y_train.T, A))
    costs.append(cost)

    dA = d_logloss(y_train.T, A)
    for layer in reversed(layers):
        dA = layer.back_propagate(dA)

plt.plot(range(epochs), costs)
plt.show()

plt.plot(range(epochs), costs)
plt.xlabel('Epochs')
plt.ylabel('Cost')
plt.show()

# layer.W.shape

x_test = x_test.reshape(10000, 784)
x_test = (x_test - np.mean(x_test))/np.std(x_test)

A = x_test.T
for layer in layers:
  A = layer.feed_forward(A)
A = A.T

count = 0
for i in range(len(y_test)):
  if y_test[i] == np.argmax(A[i]):
    count+=1
print(count/100)

# 0.0005 - 65.43% - tanh - 100epochs
# 0.0008 - 72.13% - tanh - 100epochs
# 0.0008 - 78.38% - tanh - 200epochs
# 0.0008 - 82.50% - tanh - 500epochs
# 0.0008 - 85.69% - tanh - 1500epochs
# 0.0010 - 67.47% - tanh - 200epochs

# 0.0008 - 13.49% - relu,tanh - 20epochs
# 0.0008 - 22% - relu,tanh - 2epochs